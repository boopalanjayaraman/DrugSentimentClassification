# -*- coding: utf-8 -*-
"""Drug Sentiment Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NiyXqKQGKsUJDQRbcluiHuWq15SA5HTf
"""



from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn as sklearn
import csv
import json
import nltk
import os
import random
import re
import torch
from torch import nn, optim
import torch.nn.functional as F

train_csv_path = 'drive/My Drive/Colab Notebooks/DrugData/train_original.csv'

train_df = pd.read_csv(train_csv_path, header=0)
train_df.head()

train_df.describe()

sentiment_2_set = train_df[['text', 'drug', 'sentiment']][train_df['sentiment'] == 2]
sentiment_1_set = train_df[['text', 'drug', 'sentiment']][train_df['sentiment'] == 1]
sentiment_0_set = train_df[['text', 'drug', 'sentiment']][train_df['sentiment'] == 0]
print(len(sentiment_2_set))
print(len(sentiment_1_set))
print(len(sentiment_0_set))

def sample_data_from_df(df, frac):
  if frac <= 1.0:
    temp_df = df.sample(frac = frac, replace=False)
    return temp_df
  else:
    temp_df = df.sample(frac = frac, replace=True)
    return temp_df

rows_needed_for_0 = 900
rows_needed_for_1 = 1200
rows_needed_for_2 = 1000
sentiment_2_set = sample_data_from_df(sentiment_2_set, rows_needed_for_2/len(sentiment_2_set))
sentiment_1_set = sample_data_from_df(sentiment_1_set, rows_needed_for_1/len(sentiment_1_set))
sentiment_0_set = sample_data_from_df(sentiment_0_set, rows_needed_for_2/len(sentiment_0_set))

total_train_df = pd.concat([sentiment_2_set,sentiment_1_set,sentiment_0_set])
total_train_df = total_train_df.sample(frac = 1, replace=False)

len(total_train_df)

all_texts = []

'''with open(train_csv_path, 'r', encoding='utf-8') as f:
  input_data = csv.reader(f)
  
  for row in input_data:
    all_texts.append(row)'''

all_texts = []
for _, row in total_train_df.iterrows():
  all_texts.append(row)
  
print(len(all_texts))

messages = [text[0] for text in all_texts]
sentiments = [text[2] for text in all_texts]
drug_info = [text[1] for text in all_texts]

all_drug_info = []
for index, item in train_df[['drug']].iterrows():
  all_drug_info.append(item[0].replace(' ', '-'))

index = 0
for message in messages:
  drug_name_without_space = drug_info[index].replace(' ', '-')
  messages[index] = message.replace(drug_info[index], drug_name_without_space)
  drug_info[index] = drug_name_without_space
  index += 1

print(messages[0])
print(sentiments[:20])
print(drug_info[:10])

#0 - positive
#1 - negative
#2 - neutral
from collections import Counter
sentiment_counter = Counter(sentiments)
sentiment_counter.most_common()

drugs_set = set(all_drug_info)
print(drugs_set)
print(len(drugs_set)) #original - 102

nltk.download('wordnet')

!pip install pytorch-transformers

!pip install pytorch_pretrained_bert

#from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import time
import copy
import os
from torch.utils.data import Dataset, DataLoader

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = 'all the world is a stage'
temp_tokens = tokenizer.tokenize(text)
print(temp_tokens)

class BertLayerNorm(nn.Module):
  def __init__(self, hidden_size, eps=1e-12):
    super(BertLayerNorm, self).__init__()
    self.weight = nn.Parameter(torch.ones(hidden_size))
    self.bias = nn.Parameter(torch.zeros(hidden_size))
    self.variance_epsilon = eps
    
  def forward(self, x):
    u = x.mean(-1, keepdim=True)
    s = (x - u).pow(2).mean(-1, keepdim=True)
    x = (x - u)/ torch.sqrt(s + self.variance_epsilon)
    return self.weight * x + self.bias

class BertForSequenceClassification(nn.Module):
  def __init__(self, num_labels = 2):
    super(BertForSequenceClassification, self).__init__()
    self.num_labels = num_labels
    self.bert = BertModel.from_pretrained('bert-base-uncased')
    #self.bert = BertModel.from_pretrained('bert-large-uncased')
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, num_labels)
    
    nn.init.xavier_normal_(self.classifier.weight)
    
  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
    _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
    pooled_output = self.dropout(pooled_output)
    logits = self.classifier(pooled_output)
    
    return logits
    
  def freeze_bert_encoder(self):
    for param in self.bert.parameters():
      param.requires_grad = False
      
  def unfreeze_bert_encoder(self):
    for param in self.bert.parameters():
      param.requires_grad = True

#from pytorch_transformers import BertConfig

from pytorch_pretrained_bert import BertConfig

#set up model
#bert-base-uncased: hidden_size:768 hidden_layers:12 attention_heads:12
#bert-large-uncased: hidden_size:1024 hidden_layers:24 attention_heads:16

config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, 
                    num_hidden_layers=12, num_attention_heads=12, 
                    intermediate_size=3072)

num_labels = 3
model = BertForSequenceClassification(num_labels=num_labels)

# simple test for model - logits
tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(temp_tokens)])
logits = model(tokens_tensor)
print('logits: ', logits)
print(len(messages))

context_sentences_count = 1 #meaning above 1 + below 1 + sentence with the drug

# text preprocessing function
def pre_process_text(message, drug_in_context, print_info=False):
  sentences = message.lower().split('.')
  
  indices = set()
  index = 0
  
   
  for sentence in sentences:
    if drug_in_context in sentence:
      indices.add(index)
      #add sentences before
      if index >= 1:
        indices.add(index -1)
      #add sentences afterwards
      if (index < len(sentences) - 1) and index >= 0: 
        indices.add(index+1) 
    index += 1
  
  if print_info:
    print(drug_in_context)
    print(len(sentences))
    print(indices)
   
  
  new_message = ''
  
  
  additional_context = 'About {}. '.format(drug_in_context).lower()
  new_message += additional_context
  
  for ind in indices:
    new_message += ' ' + sentences[ind]
  
  #new_message = new_message.replace(drug_in_context, 'drugincontext')
  
  return new_message

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words_set = set(stopwords.words('english'))

print(stop_words_set)
print(len(stop_words_set))

negative_polarity_stop_words = ['doesn', 'hadn', 'shan', 'nor', 'wasn', 'hasn', "mightn't", "mustn't", "wasn't", 'does', 'don', 'shouldn', 'won', "hasn't", 'couldn', "needn't", "couldn't", "won't", 'not', "hadn't", "wouldn't", "aren't", "weren't", "shan't", "isn't", 'weren', 'wouldn',  "didn't", "don't", "haven't", 'didn', 'neither', 'needn', 'haven', 'isn', 'mustn', 'mightn', "doesn't", 'ain', 'aren', "shouldn't", "never", "neva", "dint", "din", "no"]
negative_polarity_stop_words_set = set(negative_polarity_stop_words)

#remove these from stop words so they can be retained in the text to indicate negative polarity
for wrd in negative_polarity_stop_words:
  if wrd in stop_words_set:
    stop_words_set.remove(wrd)

#arrange more stop words
new_stop_words = []
    
def process_stop_words(stop_word_string, stop_words_set, new_stop_words):
  for wrd in stop_word_string.split(' '):
    if wrd not in stop_words_set:
      new_stop_words.append(wrd)
    
stop_words_new = "a about after all also always am an and any each are at be been being but by can could did do does doing else for from  had has have how i if ill i'm in into is it its i've just like many may me more most much now of only or our some something than that the their them then they thing this to up us very was way we what when where which who why will with without you your youre etc every"
stop_words_days = 'sunday monday tuesday wednesday thursday friday saturday yesterday tomorrow today afternoon morning evening tonight night'
stop_words_months = 'january jan february feb march mar april apr may june jun july jul august aug september sep october oct november nov december dec'
stop_words_numeric = 'one two three four five six seven eight nine ten twenty thirty forty fify hundred hundreds thousand thousands tens first second third fourth fifth sixth seventh eighth ninth tenth eleventh twelfth thirteenth fourteenth fifteenth twentieth hundredth thousandth'

process_stop_words(stop_words_new, stop_words_set, new_stop_words)
process_stop_words(stop_words_days, stop_words_set, new_stop_words)
process_stop_words(stop_words_months, stop_words_set, new_stop_words)
process_stop_words(stop_words_numeric, stop_words_set, new_stop_words)

for wrd in stop_words_set:
  if '\'' in wrd:
    new_stop_words.append(wrd.replace('\'', ''))

for wrd in new_stop_words:
  stop_words_set.add(wrd)
  
print(len(stop_words_set))

#import lemmatizer
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer() 
print(lemmatizer.lemmatize('corpora'))

#deal with word frequency
from collections import Counter
vocab = Counter()
for message in messages:
  words = message.split(' ')
  for word in words:
    if len(word) > 1:
      vocab.update([word])
      
print(vocab.most_common(100))
print(len(vocab.most_common()))

#words with frequency above the threshold
count = 0
word_freq_threshold = 5
for word in vocab:
  if vocab[word] >= word_freq_threshold:
    count += 1

print(count)

# pre processing tokens. 
# returns token ids 
def process_tokens(text, tokenizer, max_seq_length = 256, return_tensor=False, printInfo=False):
  #remove symbols from text
  text = remove_symbols(text)
  #tokenize the plain text 
  tokenized_text = tokenizer.tokenize(text)
  #remove stop words
  tokenized_text = remove_stopwords(tokenized_text)
  
  #clip data that is more than max length
  if len(tokenized_text) > max_seq_length:
    tokenized_text = tokenized_text[:max_seq_length]
    
  if printInfo:
    print(tokenized_text)
    
  #convert the tokens to ids
  ids_text = tokenizer.convert_tokens_to_ids(tokenized_text)
    
  #pad the data ids with zeros if the length was less than max seq length
  if len(ids_text) < max_seq_length:
    padding_array = [0] * (max_seq_length - len(ids_text))
    ids_text += padding_array
    
  if return_tensor:
    return torch.tensor(ids_text)
  
  return ids_text

def remove_stopwords(tokenized_text):
  result = []
  for token in tokenized_text:
    if token in stop_words_set:
      continue
    '''if token in negative_polarity_stop_words_set:
      result.append("negative")'''
    '''if (token not in vocab) or (vocab[token] >= word_freq_threshold):
      result.append(token)'''
    result.append(token)
  
  return result

def remove_symbols(text):
  #remove urls
  text = re.sub(r'https?:\/\/[\w\d\.\/\-\?\=#]*', '', text)
  #remove numbers
  text = re.sub(r'\d', '', text)
  #removes symbols except . ? ! [] -
  text = re.sub(r'[^!\?\-\.\w_\s]+','', text)
  #text = re.sub(r'[~`"@#$%{}:;,\+\(\)\*&<>]', '', text) 
  return text

#test support methods
test_str = 'this is a review about ocrelizumab!. he + means * drug, that, & everyone is buzzing about in the ms circles (who knew there were ms circles? well, there are ); the new drug should be approved at the end of march, so my last tysabri infusion will be in march i think we want to put you on the new '
test_tokens_1 = test_str.split(' ')
print(remove_stopwords(test_tokens_1))
print(remove_symbols(test_str))

#test preprocessing function
ind = 10
test_message = pre_process_text(messages[ind], drug_info[ind], True)
print(test_message)
tokens_output = process_tokens(test_message, tokenizer, return_tensor=True, printInfo=True)
print(tokens_output)

# sample pipeline of text 
test_tokens = tokenizer.tokenize(test_message)
test_tokens_ids = tokenizer.convert_tokens_to_ids(test_tokens)

print(test_tokens)
print(test_tokens_ids)
print(len(test_tokens_ids))

test_tokens_tensor = torch.tensor([test_tokens_ids])
print('is tensor: ', type(test_tokens_tensor) == type(torch.Tensor()))
print(test_tokens_tensor)

logits = model(tokens_tensor)
print(logits)

result = F.softmax(logits, dim=1)
print(result)

#preprocess all the messages (texts)
new_messages = []
index = 0
for message in messages:
  msg = pre_process_text(message, drug_info[index])
  msg = process_tokens(msg, tokenizer, return_tensor=True)
  new_messages.append(msg)
  index += 1
  
print(len(new_messages))
assert(len(new_messages) == len(messages))

#splitting input data for training and testing
from sklearn.model_selection import train_test_split
#X = np.array(new_messages)
X = new_messages
Y = np.array(sentiments)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)

#changing Y to one-hot encoded values
#pd.get_dummies returns data frames. 
Y_train = pd.get_dummies(Y_train).values
Y_test = pd.get_dummies(Y_test).values

max_seq_length = 256
temp_tensor_type = type(torch.Tensor())

#class that will hold training data
class TextDataset(Dataset):
  def __init__(self, x_y_data, transform=None):
    self.x_y_data = x_y_data
    self.transform = transform
    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
  def __getitem__(self, index):
    #prepare input
    input_item = self.x_y_data[0][index]
    input_tensor = input_item 
    '''if type(input_item) != temp_tensor_type:
      ids_text = process_tokens(input_item, self.tokenizer, 
                              max_seq_length)
      input_tensor = torch.tensor(ids_text)'''
    
    #prepare output
    output = self.x_y_data[1][index]
    output_tensors = [torch.from_numpy(output)]  
    
    return input_tensor, output_tensors[0]
  
  def __len__(self):
    return len(self.x_y_data[0])

#form datasets
batch_size = 16

train_lists = [X_train, Y_train]
test_lists = [X_test, Y_test]

training_dataset = TextDataset(x_y_data = train_lists)
test_dataset = TextDataset(x_y_data= test_lists)

#form data loaders dicts for training loops
dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=False, num_workers = 0),
                   'val': torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 0)}

dataset_sizes = {'train' : len(train_lists[0]),
                'val' : len(test_lists[0])}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

dataset_sizes['val']

logits_arr = []
labels_arr = []
logits_val_arr = []
#train method
def train(model, criterion, optimizer, scheduler, num_epochs = 25):
  start_time = time.time()
  print('starting training process. time: ', start_time)
  best_model_wts = copy.deepcopy(model.state_dict())
  best_loss = 100
  best_acc = 0.0
  
  #for every epoch
  for epoch in range(num_epochs):
    print('Epoch {}/{}'.format(epoch, num_epochs-1))
    print('---------')
    
    global labels_arr, logits_arr, logits_val_arr
    labels_arr, logits_arr, logits_val_arr = [], [], []
    
    #for every phase - train & validation
    for phase in ['train', 'val']:
      
      if phase == 'train':
        scheduler.step()
        model.train()
      else:
        #set model to eval state in validation mode
        model.eval()
        
      running_loss = 0.0
      accuracy = 0
      match_count = 0
      
      for features, labels in dataloaders_dict[phase]:
        features = features.to(device)
        labels = labels.to(device)
        
        #start from zero grad
        optimizer.zero_grad()
        
        #track history only in training phase
        with torch.set_grad_enabled(phase == 'train'):
          logits = model(features)
          logits = F.softmax(logits, dim=1)
          
          #[0] gives max vals, [1] gives max indices, max(dim=1) for horizontal
          labels_max_indices = torch.max(labels.float(), 1)[1] 
          
          loss = criterion(logits, labels_max_indices)
          
          #do backprop only in training phase
          if phase=='train':
            loss.backward()
            optimizer.step()
            
        #metrics
        running_loss += loss.item() * features.size(0)
        max_label_indices = torch.max(labels, 1)[1]
        max_logit_indices = torch.max(logits, 1)[1]
        matches = (max_label_indices == max_logit_indices)
        match_count += torch.sum(matches)
        
        logits_arr.append(max_logit_indices)
        labels_arr.append(max_label_indices)
        
        if phase=='val':
          logits_val_arr.append(logits)
        
      epoch_loss = running_loss / dataset_sizes[phase]
      accuracy = match_count.double() / dataset_sizes[phase]
        
      print('{} total loss: {:.4f}'.format(phase, epoch_loss))
      print('{} ACCURACY: {:.4f}'.format(phase, accuracy))
      
      #see if this is the best from last time
      if phase=='val' and epoch_loss < best_loss:
        print('saving with loss of {}'.format(epoch_loss))
        print('improved over previous best: {}'.format(best_loss))
        best_loss = epoch_loss
        best_acc = accuracy
        best_model_wts = copy.deepcopy(model.state_dict())
        torch.save(model.state_dict(), 'bert_model_state_text_classific.pth')
        
  time_elapsed = time.time() - start_time
  print('training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, 
                                                       time_elapsed % 60))
  print('best val acc.: {:.4f}, val loss: {:.4f}'.format(best_acc, best_loss))
  
  #return the model with best loss weights
  model.load_state_dict(best_model_wts)
  return model

#move model to device (gpu if any)
model.to(device)

# learning rate settings
lrLast = 0.001
lrMain = 0.00001
optimizer = optim.Adam([{"params": model.bert.parameters(), "lr": lrMain},
                       {"params": model.classifier.parameters(), "lr": lrLast}])
criterion = nn.CrossEntropyLoss()

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

# execute training
model_ft = train(model, criterion, optimizer, exp_lr_scheduler, num_epochs=20)

#save final model (best selected) to disk
torch.save(model_ft.state_dict(), 'drive/My Drive/Colab Notebooks/DrugData/bert_model_state_text_classific_best.pth')

torch.cuda.empty_cache()

#checking percentage of prediction for each label
counts = {}
corrects = {}
for index in range(len(labels_arr)):
  inner_index = 0
  for item in labels_arr[index]:
    item = int(item)
    if item in counts:
      counts[item] += 1
    else:
      counts[item] = 1
      
    if item == int(logits_arr[index][inner_index]):
      if item in corrects:
        corrects[item] += 1
      else:
        corrects[item] = 1
    inner_index += 1

print(counts)
print(corrects)

results = []
outer_index = 160
for batch_arr in logits_val_arr:
  inner_index = 0
  for arr in batch_arr:
    li = []
    for item in arr:
      li.append(float(item))#logit values
    li.append(int(labels_arr[outer_index][inner_index]))#original
    li.append(int(logits_arr[outer_index][inner_index]))#predicted
    results.append(li)
    inner_index +=1
  outer_index += 1
  
print(len(results))



len(labels_arr)

np.savetxt('drive/My Drive/Colab Notebooks/DrugData/model_labels_vs_logits_3200_1.csv', np.array(results), delimiter=",")

